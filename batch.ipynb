{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-0.19.1-py2.py3-none-any.whl (17 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-0.19.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ndjson in /Users/yottan/.pyenv/versions/anaconda3-2020.07/envs/streamlit-custom/lib/python3.8/site-packages (0.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ndjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting demoji\n",
      "  Using cached demoji-1.1.0-py3-none-any.whl (42 kB)\n",
      "Installing collected packages: demoji\n",
      "Successfully installed demoji-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c258806e9f33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msentence_vectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdotenv_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.env'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdotenv_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import smtplib\n",
    "import os\n",
    "import json\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.application import MIMEApplication\n",
    "from email.utils import formatdate\n",
    "from os.path import join, dirname\n",
    "import time\n",
    "\n",
    "msg = MIMEMultipart()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ndjson\n",
    "from dotenv import load_dotenv\n",
    "import sentence_vectorizer\n",
    "import functools\n",
    "import demoji\n",
    "from ja_sentence_segmenter.common.pipeline import make_pipeline\n",
    "from ja_sentence_segmenter.concatenate.simple_concatenator import concatenate_matching\n",
    "from ja_sentence_segmenter.normalize.neologd_normalizer import normalize\n",
    "from ja_sentence_segmenter.split.simple_splitter import split_newline, split_punctuation\n",
    "\n",
    "import liking_users\n",
    "import liked_tweets\n",
    "import sentence_vectorizer\n",
    "\n",
    "# dotenv_path = join(dirname(__file__), '.env')\n",
    "# load_dotenv(dotenv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client():\n",
    "\n",
    "    DIR = 'data/mail_body.txt'\n",
    "\n",
    "    def __init__(self, client_data):\n",
    "\n",
    "        self.client_data = client_data\n",
    "        self.execution_time = 0\n",
    "        self.target_user = []\n",
    "        self.liked_tweet = []\n",
    "        self.degree = []\n",
    "\n",
    "    def extract_data(self):\n",
    "        \"\"\"ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è¾æ›¸ã‹ã‚‰å€¤ã‚’ãã‚Œãã‚ŒæŠ½å‡º\n",
    "        \"\"\"\n",
    "        # é€ä¿¡å…ˆã‚’æŠ½å‡º    \n",
    "        self.mail_address = self.client_data['mail_address']\n",
    "        # å®›åã‚’æŠ½å‡º\n",
    "        self.name = self.client_data['name']\n",
    "        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º\n",
    "        self.keyword = self.client_data['keyword']\n",
    "        # ãƒ„ã‚¤ãƒƒã‚¿ãƒ¼IDã‚’æŠ½å‡º\n",
    "        self.target_tweet_id = self.client_data['twitter_id']\n",
    "        # ãƒ„ã‚¤ãƒƒã‚¿ãƒ¼æœ¬æ–‡ã‚’æŠ½å‡º\n",
    "        df_tweets = pd.DataFrame(self.client_data['tweets'])\n",
    "        self.target_tweet = df_tweets.iloc[:,1][df_tweets.iloc[:,2]==self.target_tweet_id].values[0]\n",
    "\n",
    "    def send_email(self):\n",
    "        \"\"\"ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå®›ã«ãƒ¡ãƒ¼ãƒ«é€ä¿¡\n",
    "        \"\"\"\n",
    "        send_address = os.environ.get('MAIL_ADDRESS')\n",
    "        password = os.environ.get('MAIL_PASSWORD')\n",
    "\n",
    "        with open(DIR, encoding = \"utf-8\") as f:\n",
    "            body_temp = f.read()\n",
    "\n",
    "        subject = 'Twitteråˆ†æçµæœ'\n",
    "        from_address = send_address\n",
    "        to_address = self.mail_address\n",
    "\n",
    "        # SMTPã‚µãƒ¼ãƒã«æ¥ç¶š\n",
    "        smtpobj = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "        smtpobj.starttls()\n",
    "        smtpobj.login(send_address, password)\n",
    "\n",
    "        # ãƒ¡ãƒ¼ãƒ«ä½œæˆ\n",
    "        msg = MIMEMultipart()\n",
    "        msg['Subject'] = subject\n",
    "        msg['From'] = from_address\n",
    "        msg['To'] = to_address\n",
    "        msg['Date'] = formatdate()\n",
    "\n",
    "        # ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡\n",
    "        # ä¸€ç•ªå¤§ãã„é¡ä¼¼åº¦ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "        max_idx = self.degree.index(max(self.degree))\n",
    "        \n",
    "        body_text = body_temp.format(name=self.name,\\\n",
    "            keyword=self.keyword, target_tweet=self.target_tweet,\\\n",
    "            target_user=self.target_user[max_idx], liked_tweet=self.liked_tweet[max_idx],\\\n",
    "            degree=max(self.degree), execution_time=self.execution_time)\n",
    "        body = MIMEText(body_text)\n",
    "        msg.attach(body)\n",
    "\n",
    "        # ä½œæˆã—ãŸãƒ¡ãƒ¼ãƒ«ã‚’é€ä¿¡\n",
    "        smtpobj.send_message(msg)\n",
    "        smtpobj.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Request returned an error: 403 {\"client_id\":\"22371308\",\"detail\":\"When authenticating requests to the Twitter API v2 endpoints, you must use keys and tokens from a Twitter developer App that is attached to a Project. You can create a project via the developer portal.\",\"registration_url\":\"https://developer.twitter.com/en/docs/projects/overview\",\"title\":\"Client Forbidden\",\"required_enrollment\":\"Standard Basic\",\"reason\":\"client-not-enrolled\",\"type\":\"https://api.twitter.com/2/problems/client-forbidden\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-304607267cb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-304607267cb5>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(dir)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mdf_target_segments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize_target_segments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_target_segments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mdf_users\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_liking_users\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_tweet_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mdf_liked_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_liking_users_liked_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_tweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mcandidate_users\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_liked_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-e74fe83e2e31>\u001b[0m in \u001b[0;36mget_liking_users\u001b[0;34m(target_tweet_id)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \"\"\"\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ„ã‚¤ãƒ¼ãƒˆIDã‹ã‚‰ã„ã„ã­ã—ãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’æ¤œç´¢\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mjson_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mliking_users\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_users\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_tweet_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# ã„ã„ã­ã—ãŸãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ \u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_response\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Aidemy/reserch_target_twitter/liking_users.py\u001b[0m in \u001b[0;36mget_users\u001b[0;34m(target_id)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_users\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet_fields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mjson_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnect_to_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Aidemy/reserch_target_twitter/liking_users.py\u001b[0m in \u001b[0;36mconnect_to_endpoint\u001b[0;34m(url, user_fields)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         raise Exception(\n\u001b[0m\u001b[1;32m     47\u001b[0m             \"Request returned an error: {} {}\".format(\n\u001b[1;32m     48\u001b[0m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Request returned an error: 403 {\"client_id\":\"22371308\",\"detail\":\"When authenticating requests to the Twitter API v2 endpoints, you must use keys and tokens from a Twitter developer App that is attached to a Project. You can create a project via the developer portal.\",\"registration_url\":\"https://developer.twitter.com/en/docs/projects/overview\",\"title\":\"Client Forbidden\",\"required_enrollment\":\"Standard Basic\",\"reason\":\"client-not-enrolled\",\"type\":\"https://api.twitter.com/2/problems/client-forbidden\"}"
     ]
    }
   ],
   "source": [
    "DIR = 'data/db_tweets.ndjson'\n",
    "\n",
    "def main(dir):\n",
    "    \"\"\"ãƒ¡ã‚¤ãƒ³\n",
    "    \"\"\"\n",
    "    client_data_list = []\n",
    "\n",
    "    with open(dir, 'r+') as json_file:\n",
    "        # ç¾åœ¨DBã«ã‚ã‚‹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—\n",
    "        json_client_data = ndjson.load(json_file)\n",
    "        # DBã®å†…å®¹ã‚’ç©ºã«ã™ã‚‹\n",
    "        # json_file.truncate(0) \n",
    "\n",
    "        for data in json_client_data:\n",
    "            # é–‹å§‹æ™‚é–“\n",
    "            start_time = time.perf_counter()\n",
    "            # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–\n",
    "            client = Client(data)\n",
    "            # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "            client.extract_data()\n",
    "            \n",
    "            df_target_segments = segment_target_tweet(client.target_tweet)\n",
    "            df_target_segments = vectorize_target_segments(df_target_segments)\n",
    "\n",
    "            df_users = get_liking_users(client.target_tweet_id)\n",
    "            df_liked_tweets = get_liking_users_liked_tweets(df_users, client.target_tweet)\n",
    "            candidate_users = select_tweets(client.keyword, df_liked_tweets)\n",
    "            candidate_users_segments = segment_tweet(candidate_users)\n",
    "            df_candidate_users = vectorize_candidate_segments(candidate_users_segments)\n",
    "            #dic_degree, dic_index = calculate_cosine_similarity(df_target_segments, df_candidate_users)\n",
    "            derived_result(client, df_target_segments, df_candidate_users, df_users, df_liked_tweets)\n",
    "            \n",
    "            client.send_email()\n",
    "            # çµ‚äº†æ™‚é–“\n",
    "            execution_time = time.perf_counter() - start_time\n",
    "            client.execution_time = execution_time\n",
    "\n",
    "    return client\n",
    "                 \n",
    "client = main(DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.75378513800388\n"
     ]
    }
   ],
   "source": [
    "print(client.execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#udemy'"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ï¼š0.901680052280426\n",
      "\n",
      "#ä»Šæ—¥ã®ç©ã¿ä¸Šã’ 11/9\n",
      "\n",
      "#udemy ã‚¦ã‚§ãƒ–é–‹ç™ºå…¥é–€å®Œå…¨æ”»ç•¥ã‚³ãƒ¼ã‚¹\n",
      "JavaScriptå…¥é–€ã€€32%çµ‚äº†\n",
      "\n",
      "è¬›åº§ã‚’å¤‰ãˆã¦ã€4åº¦ç›®ã®JSæŒ‘æˆ¦\n",
      "\n",
      "æ˜¨æ—¥ã€è‡ªç¤¾ã‚·ã‚¹ãƒ†ãƒ ã®ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‚’ä½œã‚Šã¾ã—ãŸãŒ\n",
      "ã“ã®ãƒšãƒ¼ã‚¸ãŒãƒšãƒ¼ã‚¸ã§çµ‚ã‚ã‚‰ãšã€æ©Ÿèƒ½ã‚’å®Ÿè£…ã•ã›ã¦ã€å¿…ãšç”Ÿç”£æ€§ã¨ã—ã¦å½¹ç«‹ã¦ã¾ã™ğŸ’ª\n",
      "\n",
      "#é§†ã‘å‡ºã—ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ #ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°åˆå¿ƒè€… https://t.co/Do7aQ2cn6Q\n"
     ]
    }
   ],
   "source": [
    "maxidx = client.degree.index(max(client.degree))\n",
    "print(f'é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ï¼š{max(client.degree)}')\n",
    "print()\n",
    "print(client.liked_tweet[maxidx]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for degree_lst, index_lst in zip(dic_degree.values(), dic_index.values()):\n",
    "        \n",
    "    client.degree.append(max(degree_lst))\n",
    "    max_idx = degree_lst.index(max(degree_lst))\n",
    "    target_user_id = df_candidate_users.iloc[max_idx].name\n",
    "    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼å\n",
    "    print(df_users['username'][df_users['id']==target_user_id].values)\n",
    "    client.target_user.append(df_users['username'][df_users['id']==target_user_id])\n",
    "    # ãƒ„ã‚¤ãƒ¼ãƒˆæ–­ç‰‡ã‚’æŠ½å‡º\n",
    "    segment_index = index_lst[max_idx]\n",
    "    segment = df_candidate_users.loc[target_user_id,'text'][segment_index]\n",
    "    # ãƒ„ã‚¤ãƒ¼ãƒˆæ–­ç‰‡ã‚’å«ã‚€ãƒ„ã‚¤ãƒ¼ãƒˆæœ¬æ–‡ã‚’æŠ½å‡º\n",
    "    df_mask = df_liked_tweets[target_user_id].str.contains(segment)\n",
    "    target_row = df_mask.first_valid_index()\n",
    "    # print(target_row)\n",
    "    # print(df_liked_tweets[target_user_id].iloc[target_row])\n",
    "    client.liked_tweet.append(df_liked_tweets[target_user_id].iloc[target_row])\n",
    "\n",
    "    print(client.degree)\n",
    "    print(client.target_user)\n",
    "    print(client.liked_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_liking_users(target_tweet_id):\n",
    "    \"\"\"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ„ã‚¤ãƒ¼ãƒˆã«ã„ã„ã­ã—ãŸãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã§è¿”ã™\n",
    "    \"\"\"\n",
    "    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ„ã‚¤ãƒ¼ãƒˆIDã‹ã‚‰ã„ã„ã­ã—ãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’æ¤œç´¢\n",
    "    json_response = liking_users.get_users(target_tweet_id)\n",
    "    # ã„ã„ã­ã—ãŸãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ \n",
    "    return pd.json_normalize(json_response['data'])\n",
    "\n",
    "def get_liking_users_liked_tweets(df_users, target_tweet):\n",
    "    \"\"\"ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã„ã„ã­ã—ãŸãƒ„ã‚¤ãƒ¼ãƒˆã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¿”ã™\n",
    "    \"\"\"    \n",
    "    # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‹ã‚‰ã„ã„ã­ã—ãŸãƒ„ã‚¤ãƒ¼ãƒˆã‚’æ¤œç´¢\n",
    "    dic_liked_tweets = liked_tweets.get_liked_tweets(df_users['id'])\n",
    "    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®è¾æ›¸ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ãŸãƒãƒ«ãƒã‚«ãƒ©ãƒ ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ\n",
    "    df_liked_tweets = pd.concat({k: pd.DataFrame(v['data']) for k, v in dic_liked_tweets.items()}, \\\n",
    "        axis=0).unstack(0).swaplevel(1,0, axis=1).sort_index(axis=1)\n",
    "    # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’æŠ½å‡º    \n",
    "    user_ids = df_liked_tweets.columns.get_level_values(0).unique()\n",
    "\n",
    "    tweets_text_list = []\n",
    "    for ids in user_ids:\n",
    "        user_texts = df_liked_tweets[ids]['text'].copy()\n",
    "        # ã„ã„ã­ã—ãŸãƒ„ã‚¤ãƒ¼ãƒˆã«ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ„ã‚¤ãƒ¼ãƒˆãŒå«ã¾ã‚Œã¦ã„ãŸã‚‰NANã«ç½®æ›ã™ã‚‹\n",
    "        user_texts.replace(target_tweet, np.nan, inplace=True)\n",
    "        tweets_text_list.append(user_texts)\n",
    "        # ã„ã„ã­ã—ãŸãƒ„ã‚¤ãƒ¼ãƒˆã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ \n",
    "    df_tweets_text = pd.DataFrame(tweets_text_list, index=user_ids)\n",
    "    return df_tweets_text.T     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã„ã„ã­ãƒ„ã‚¤ãƒ¼ãƒˆã¸ã®å‰å‡¦ç†ç”¨\n",
    "\n",
    "def select_tweets(keyword, df_liked_tweets):\n",
    "    \"\"\"ãƒ„ã‚¤ãƒ¼ãƒˆã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€æ–‡ç« ã‚’ã‚‚ã¤ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚·ãƒªãƒ¼ã‚ºã‚’ä½œæˆ\n",
    "    \"\"\"  \n",
    "    for col in df_liked_tweets.columns:\n",
    "        flag = df_liked_tweets[col].str.contains(keyword, na=False)\n",
    "        df_liked_tweets.loc[~flag, col] = np.nan\n",
    "\n",
    "    # NANä»¥å¤–ã®å€¤ã‚’æŒã¤ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’ç‰¹å®šã™ã‚‹ï¼ˆPOSTã™ã‚‹å€™è£œï¼‰\n",
    "    exist_flag = df_liked_tweets.count() != 0\n",
    "    user_name = df_liked_tweets.T[exist_flag].index\n",
    "\n",
    "    # POSTã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼å€™è£œã”ã¨ã«é–¢é€£ãƒ„ã‚¤ãƒ¼ãƒˆæ–‡ã‚’å…¨ã¦çµåˆ\n",
    "    candidate_users = df_liked_tweets[user_name].T\n",
    "    # å¾Œã§æ–‡ã‚’åŒºåˆ‡ã‚‹åŒºåˆ‡ã‚Šã®ãŸã‚ãƒ„ã‚¤ãƒ¼ãƒˆæ–‡ã®æ–‡æœ«ã«'ã€‚'ã‚’è¿½åŠ \n",
    "    candidate_users += 'ã€‚'\n",
    "    # ãƒ„ã‚¤ãƒ¼ãƒˆæ–‡ã‚’çµåˆ\n",
    "    return candidate_users.iloc[:,0].str.cat([candidate_users.iloc[:,col] for col in candidate_users.columns[1:]], na_rep='')\n",
    "\n",
    "def segment_tweet(candidate_users):\n",
    "    \"\"\"ãƒ„ã‚¤ãƒ¼ãƒˆã‚’æ–‡å˜ä½ã§ã‚»ãƒ‘ãƒ¬ãƒ¼ãƒˆã—ãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚·ãƒªãƒ¼ã‚ºã‚’è¿”ã™\n",
    "    \"\"\"  \n",
    "    split_punc2 = functools.partial(split_punctuation, punctuations=r\"ã€‚!?\")\n",
    "    concat_tail_no = functools.partial(concatenate_matching, former_matching_rule=r\"^(?P<result>.+)(ã®)$\", remove_former_matched=False)\n",
    "    segmenter = make_pipeline(normalize, split_newline, concat_tail_no, split_punc2)\n",
    "\n",
    "    sentence_list = []\n",
    "    for text in candidate_users:\n",
    "        sentence_list.append(list(segmenter(text)))\n",
    "\n",
    "    candidate_users_segments = pd.Series(sentence_list, index=candidate_users.index, name='text')\n",
    "\n",
    "    for i in range(len(candidate_users_segments)):\n",
    "        candidate_users_segments[i] = [demoji.replace(string=seg, repl='') for seg in candidate_users_segments[i] if 'ã€‚' != seg]\n",
    "\n",
    "    return candidate_users_segments\n",
    "\n",
    "def vectorize_candidate_segments(candidate_users_segments):\n",
    "    \"\"\"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚·ãƒªãƒ¼ã‚ºã«æ–‡å˜ä½ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿½åŠ ã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¿”ã™\n",
    "    \"\"\"  \n",
    "    # ãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾ã‚’ç”Ÿæˆã™ã‚‹\n",
    "    BSV = sentence_vectorizer.BertSequenceVectorizer()\n",
    "    vectors_list = [pd.Series(text).map(lambda x: BSV.vectorize(x)) for text in candidate_users_segments]\n",
    "    sr_vectors = pd.Series(vectors_list, index=candidate_users_segments.index, name='vector')\n",
    "    return pd.concat([candidate_users_segments, sr_vectors], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ„ã‚¤ãƒ¼ãƒˆã¸ã®å‰å‡¦ç†\n",
    "\n",
    "def segment_target_tweet(target_tweet):\n",
    "    \"\"\"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ„ã‚¤ãƒ¼ãƒˆã‚’æ–‡å˜ä½ã§ã‚»ãƒ‘ãƒ¬ãƒ¼ãƒˆã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¿”ã™\n",
    "    \"\"\"  \n",
    "    split_punc2 = functools.partial(split_punctuation, punctuations=r\"ã€‚!?\")\n",
    "    concat_tail_no = functools.partial(concatenate_matching, former_matching_rule=r\"^(?P<result>.+)(ã®)$\", remove_former_matched=False)\n",
    "    segmenter = make_pipeline(normalize, split_newline, concat_tail_no, split_punc2)\n",
    "\n",
    "    return pd.DataFrame(segmenter(target_tweet), columns=['text'])\n",
    "\n",
    "def vectorize_target_segments(df_target_segments):\n",
    "    \"\"\"æ–‡å˜ä½ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿½åŠ \n",
    "    \"\"\"  \n",
    "    BSV = sentence_vectorizer.BertSequenceVectorizer()\n",
    "    df_target_segments['vector'] = df_target_segments['text'].map(lambda x: BSV.vectorize(x))\n",
    "    return df_target_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(df_target_segments, df_candidate_users):\n",
    "    \"\"\"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ„ã‚¤ãƒ¼ãƒˆã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ„ã‚¤ãƒ¼ãƒˆã®é¡ä¼¼åº¦ã‚’è¨ˆç®—\n",
    "    \"\"\"     \n",
    "\n",
    "    # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®é«˜ã„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ç®—å‡º\n",
    "    dic_degree = {}\n",
    "    dic_index = {}\n",
    "\n",
    "    for vec in df_candidate_users['vector']:\n",
    "\n",
    "        join = df_target_segments['vector'].append(vec)\n",
    "        for idx in range(len(df_target_segments)):\n",
    "            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ„ã‚¤ãƒ¼ãƒˆã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’é¸æŠã™ã‚‹ãƒã‚¹ã‚¯\n",
    "            mask = np.ones(len(join), dtype=bool)\n",
    "            mask[:len(df_target_segments)+1] = False\n",
    "            mask[idx] = True\n",
    "            # é¡ä¼¼åº¦è¡Œåˆ—\n",
    "            matrix = sentence_vectorizer.cos_sim_matrix(np.stack(join[mask]))\n",
    "            #print(np.sort(matrix[0])[::-1])\n",
    "            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã«æœ€å¤§ã®é¡ä¼¼åº¦ã¨ãã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ„ã‚¤ãƒ¼ãƒˆã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—\n",
    "            degree = np.sort(matrix[0])[::-1][1]\n",
    "            index = np.argsort(matrix[0])[::-1][1]\n",
    "\n",
    "            dic_degree.setdefault(idx, []).append(degree)\n",
    "            dic_index.setdefault(idx, []).append(index)\n",
    "\n",
    "    return dic_degree, dic_index\n",
    "\n",
    "def derived_result(client, df_target_segments, df_candidate_users, df_users, df_liked_tweets):\n",
    "    \"\"\"ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼åã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã„ã„ã­ã—ãŸé–¢é€£åº¦ã®é«˜ã„ãƒ„ã‚¤ãƒ¼ãƒˆã€é–¢é€£ã‚¹ã‚³ã‚¢ï¼ˆ0ã€œ1ï¼‰ã‚’è¿”ã™\n",
    "    \"\"\"\n",
    "    dic_degree, dic_index = calculate_cosine_similarity(df_target_segments, df_candidate_users)\n",
    "    for degree_lst, index_lst in zip(dic_degree.values(), dic_index.values()):\n",
    "        # æœ€å¤§ã®é–¢é€£åº¦ã‚¹ã‚³ã‚¢\n",
    "        client.degree.append(max(degree_lst))\n",
    "        max_idx = degree_lst.index(max(degree_lst))\n",
    "        target_user_id = df_candidate_users.iloc[max_idx].name\n",
    "        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼å\n",
    "        client.target_user.append(df_users['username'][df_users['id']==target_user_id].values)\n",
    "        segment_index = index_lst[max_idx]\n",
    "        # ãƒ„ã‚¤ãƒ¼ãƒˆæ–­ç‰‡ã‚’æŠ½å‡º\n",
    "        segment = df_candidate_users.loc[target_user_id,'text'][segment_index]\n",
    "        # ãƒ„ã‚¤ãƒ¼ãƒˆæ–­ç‰‡ã‚’å«ã‚€ãƒ„ã‚¤ãƒ¼ãƒˆæœ¬æ–‡ã‚’æŠ½å‡º\n",
    "        df_mask = df_liked_tweets[target_user_id].str.contains(segment)\n",
    "        target_row = df_mask.first_valid_index()\n",
    "        client.liked_tweet.append(df_liked_tweets.loc[target_row, target_user_id])\n",
    "\n",
    "    print(client.degree, client.target_user, client.liked_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        self.target_user = []\n",
    "        self.liked_tweet = []\n",
    "        self.degree = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1175285742001475586</th>\n",
       "      <th>1260973997845393408</th>\n",
       "      <th>1279267537629278211</th>\n",
       "      <th>1282640602396364802</th>\n",
       "      <th>1290624958603829248</th>\n",
       "      <th>1297906406155104256</th>\n",
       "      <th>1356973982</th>\n",
       "      <th>1414520142</th>\n",
       "      <th>1415640388241555461</th>\n",
       "      <th>1449028461310332936</th>\n",
       "      <th>963753537053143042</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ã¾ã æ–°è¦ç”³ã—è¾¼ã¿ã¯åœæ­¢ä¸­ã€‚\\nå†ä¼šæ™‚ã«ã¯æƒ…å ±å…±æœ‰ã—ã¾ã™ã€‚\\n\\n#é§†ã‘å‡ºã—ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã¨ç¹‹ãŒã‚Š...</td>\n",
       "      <td>ã€Œãƒãƒƒãƒãƒ³ã‚°ã‚¢ãƒ—ãƒªã€åˆæ ¼ã§ãã¾ã—ãŸï¼ https://t.co/13jNVhiZmp #Re...</td>\n",
       "      <td>AR/VRã«ã¯å¤§ããªãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ãŒã‚ã‚‹ã¨æ€ã„ã¾ã™ãŒã€Oculus Questã«ç‰¹åŒ–ã—ãŸã‚³ãƒ³ãƒ†...</td>\n",
       "      <td>ç§ã¨è¿«ã•ã‚“ï¼ˆ@yuki_99_sï¼‰ã®YouTubeè£ãƒã‚¦ãƒã‚¦Brainã§ã™ãŒã€3ã‚«æœˆä»¥ä¸ŠçµŒã£...</td>\n",
       "      <td>2021/11/08 8æ—¥ç›® ä½œæ¥­æ™‚é–“:1h30m\\n\\nãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–Dog3ã¾ã§å®Œæˆï¼\\n...</td>\n",
       "      <td>ã€#tomoya_yoshimoto_in_isub #isubã€‘from slack\\nå¿«...</td>\n",
       "      <td>é€†é‹å‹•å­¦ã«æ‚©ã‚€ãƒ»ãƒ»ãƒ»\\nhttps://t.co/U1vksBjNQi\\n\\n#Arduin...</td>\n",
       "      <td>é›¢è·ç‡ã®é«˜ã„ä¼šç¤¾ã£ã¦ã™ã”ã„ãƒãƒ£ãƒ³ã‚¹ã ã£ãŸã‚Šã™ã‚‹ã€‚\\n\\nå…ˆè¼©ã‚„ä¸Šå¸ãŒã©ã‚“ã©ã‚“è¾ã‚ã¦ã„ãã®ã§ã€...</td>\n",
       "      <td>#ä»Šæ—¥ã®ç©ã¿ä¸Šã’\\nä»Šæ—¥ã¯ã‚¨ãƒŠã‚¸ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ã®ç¼¶ã‚’ç©ã¿ã¾ã—ãŸâ€¼ï¸ ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚¹ã¨ã—ã¦ç‹¬ç«‹ã—ã¦ã„...</td>\n",
       "      <td>æ˜¨æ—¥ä»Šæ—¥ã¨åœé›»&amp;amp;ãƒãƒƒãƒˆçµ¶ä¸èª¿ã§ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°å­¦ç¿’ã¯ãŠã‚ã‹ã€SNSã•ãˆã¾ã¨ã‚‚ã«ã§ããªã„...</td>\n",
       "      <td>Railsã€€Modelã§æ•´æ•°å€¤ã«é™å®šã—ãŸã‘ã°\\nvalidates :user_id, nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@banburero001 å˜˜ã¨ã„ã†ã‹ã€å•†æå±‹ãŒäººã‚’é¨™ã™ãŸã‚ã®ã‚¨ã‚µã§ã™ã­ã€‚\\næœˆåâ—‹æ¡ï¼ï¼ã¨...</td>\n",
       "      <td>æœè£…ã«è‡ªä¿¡ãŒãªã„ #ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ¼ å¿…è¦‹ï¼\\n#ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ å‘ã‘ãƒ–ãƒ©ãƒ³ãƒ‰ï¼\\n\\n#ãƒ—ãƒ­ã‚°ãƒ©ãƒŸ...</td>\n",
       "      <td>#ã‚®ãƒ¼ã‚¯ãƒã‚¦ã‚¹ å“å·ãƒ»äº”åç”°ãƒ»å¤§å´ã§ã¯ä½äººã‚’å‹Ÿé›†ä¸­!\\n\\nã‚³ãƒ³ã‚»ãƒ—ãƒˆã¯ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢åŒå£«(å¿—æœ›...</td>\n",
       "      <td>ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™â˜€ï¸\\n\\nã€Œèª°ã§ã‚‚ã§ãã‚‹ã“ã¨ã€ã‚’ã€Œèª°ã‚‚ã§ããªã„ãã‚‰ã„ç¶šã‘ã‚‹ã€ãŒæˆåŠŸã®ã‚³ãƒ„...</td>\n",
       "      <td>#ãƒ‡ã‚¤ãƒˆãƒ©\\nåˆ†ã‹ã‚‰ãªã„ã¨ã“ã‚ã‚¬ãƒ³ã‚¬ãƒ³å‘Ÿã„ã¦ã„ãã¾ã™ğŸ˜µâ€ğŸ’«\\n\\nä¸­ç´šç·¨ã§ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–ã®\\n...</td>\n",
       "      <td>è‡ªåˆ†ã¯ã¿ã‚“ãªã‚’åŠ©ã‘ãŸã„ã¨è¨€ã£ã¦ã‚‚ã€çµå±€èª°ã‚‚äººãŒåŠ©ã‘ã‚’æ±‚ã‚ã¦æ¥ãªã„ã®ãªã‚‰ãã‚Œã¯ã¾ãšãã®äººã«äººã‚’...</td>\n",
       "      <td>é ­æ‚ªã„å¥´ã¯ #ã‚²ãƒ¼ãƒ  ã ã‚ã†ãŒæ”»ç•¥æ³•è¦‹ã¦ã‚Šã‚ƒè‰¯ã„\\n\\n#ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚° ãªã©å°‚é–€çŸ¥è­˜ã«ã—ã¦...</td>\n",
       "      <td>æŒã¤ã¹ãã¯çµ¶å¯¾çš„ãªå±æ©Ÿæ„Ÿ\\nä¼šç¤¾å“¡æ™‚ä»£ã«ã¯çµ‚èº«é›‡ç”¨ã§å®‰æ³°ã¨æ€ã‚ãªã„ã“ã¨ã€‚å¸¸ã«ç–‘å•ç¬¦ã‚’ã‚‚ã¡10...</td>\n",
       "      <td>[From ï¾•ï¾ƒï¾ï¾€ï¾ï½¸ï¾ï½«bot]\\nğŸ£ #ä»Šæ—¥ã®ç©ã¿ä¸Šã’\\n#é§†ã‘å‡ºã—ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã¨ç¹‹ãŒã‚Š...</td>\n",
       "      <td>Progateã§ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°å‹‰å¼·å§‹ã‚ã¾ã—ãŸï¼ï¼\\nHTMLï¼†CSSã€€åˆç´šç·¨ã‚„ã£ã¦ã¾ã™ï¼\\n...</td>\n",
       "      <td>ğŸ™‡ğŸ»â€â™‚ï¸å¾¡ç¤¼ğŸ™‡ğŸ»â€â™‚ï¸\\n\\nã‚¯ãƒ©ã‚¦ãƒ‰ãƒ¯ãƒ¼ã‚¯ã‚¹ã§ä½•åº¦å¿œå‹Ÿã—ã¦ã‚‚ãƒ€ãƒ¡ãªäººã®ãŸã‚ã®Brainï¼ˆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ã‹ã”ã‚‚ã #50 èªè¨¼ãƒ‡ãƒã‚¤ã‚¹ç‰¹é›† (FIDO2 ã¨ã‹ Yubikey ã¨ã‹) / http...</td>\n",
       "      <td>Näºˆå‚™æ ¡ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°å…¥é–€ã‚³ãƒ¼ã‚¹ã¯ã‚´ãƒ¼ãƒ«ãŒã€Webã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã¨ã—ã¦å†…å®šã‚‚ã‚‰ã£ã¦åƒã‘ã‚‹ã€ãƒ¬ãƒ™...</td>\n",
       "      <td>ã€2021å¹´æœ€æ–°ç‰ˆã€‘\\n å›½å†…æœ€å¤§æ‰‹ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚¹ã‚¯ãƒ¼ãƒ«\\nä¾ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å¡¾ã®æ°—ã«ãªã‚‹æ–™é‡‘...</td>\n",
       "      <td>è‰²ã€…ã¨è¨˜äº‹ã«ã—ã¦å…±æœ‰ã—ã¦ã„ããŸã„çŸ¥è­˜ãŒã“ã“1å¹´ã§ãŸãã•ã‚“èº«ã«ã¤ã„ãŸæ°—ãŒã—ã¾ã™ï¼\\n\\nã„ã¾è¡Œ...</td>\n",
       "      <td>å®Ÿæˆ¦ã—ãŸä¸­ã«ã‹ãªã‚ŠåŠ¹æœçš„ãªå‹‰å¼·ã¯\\nã‚´ãƒ¼ãƒ«ã‹ã‚‰å§‹ã‚ã‚‹äº‹ã§ã™ï¼\\nè³‡æ ¼ãªã‚‰éå»å•ã‹ã‚‰è§£ã\\nåˆ¶...</td>\n",
       "      <td>ãƒˆãƒªãƒ‹ã‚¿ã‚¹ãŠä»•äº‹æƒ…å ±â™ªé«˜é¡æ¡ˆä»¶ã‚ã‚Šâ™ªã‚¹ã‚­ãƒ«ã‚¢ãƒƒãƒ—â™ªé¸ã¹ã‚‹æ¡ˆä»¶â™ªã€€https://t.co/t...</td>\n",
       "      <td>2021-11-08 17:00:02\\nä»™å°å¸‚\\nç¾åœ¨ã®å¤©å€™ï¼šæ‰€ã«ã‚ˆã‚Šæ›‡ã‚Š\\nã€€ã€€ã€€æ°—æ¸©ï¼š...</td>\n",
       "      <td>#Qiita æŠ•ç¨¿ã—ã¾ã—ãŸã€‚\\n\\nhttps://t.co/cTLwZgbcWY\\n\\n#...</td>\n",
       "      <td>ãƒ–ãƒ³ãƒ»ãƒ¬ãƒ¢ãƒ³ã‚°ãƒ©ã‚¹ ãƒã‚­ãƒ³ã€ãƒ™ãƒˆãƒ‹ãƒ£ãƒƒãƒˆã€æ–°å®¿å¾¡è‹‘ #ãŠã¯æˆ¦31102nk #ãŠã¤ã‹ã‚Œæˆ¦éšŠ1...</td>\n",
       "      <td>PHPã®è³ªå•ã«ã¾ã ç­”ãˆã‚‰ã‚Œãªãã¦æ‚²ã—ã„ğŸ’¦\\nçŸ¥ã‚Šåˆã„ã§PHPã®ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã•ã‚“ã¨ã‹ã„ã‚Œã°ã‚ˆã‹ã£...</td>\n",
       "      <td>ï¼\\nğŸ”°æœªçµŒé¨“è€…å¿…è¦‹ï¼\\nï¼¼\\n\\næœªçµŒé¨“ã‹ã‚‰ã®ITè»¢è·ã¯20ä»£ãŒæœ‰åˆ©ï¼ğŸš€\\nè»¢è·ã™ã‚‹ãŸã‚ã«...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 1175285742001475586  \\\n",
       "0  ã¾ã æ–°è¦ç”³ã—è¾¼ã¿ã¯åœæ­¢ä¸­ã€‚\\nå†ä¼šæ™‚ã«ã¯æƒ…å ±å…±æœ‰ã—ã¾ã™ã€‚\\n\\n#é§†ã‘å‡ºã—ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã¨ç¹‹ãŒã‚Š...   \n",
       "1  @banburero001 å˜˜ã¨ã„ã†ã‹ã€å•†æå±‹ãŒäººã‚’é¨™ã™ãŸã‚ã®ã‚¨ã‚µã§ã™ã­ã€‚\\næœˆåâ—‹æ¡ï¼ï¼ã¨...   \n",
       "2  ã‹ã”ã‚‚ã #50 èªè¨¼ãƒ‡ãƒã‚¤ã‚¹ç‰¹é›† (FIDO2 ã¨ã‹ Yubikey ã¨ã‹) / http...   \n",
       "\n",
       "                                 1260973997845393408  \\\n",
       "0  ã€Œãƒãƒƒãƒãƒ³ã‚°ã‚¢ãƒ—ãƒªã€åˆæ ¼ã§ãã¾ã—ãŸï¼ https://t.co/13jNVhiZmp #Re...   \n",
       "1  æœè£…ã«è‡ªä¿¡ãŒãªã„ #ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ¼ å¿…è¦‹ï¼\\n#ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ å‘ã‘ãƒ–ãƒ©ãƒ³ãƒ‰ï¼\\n\\n#ãƒ—ãƒ­ã‚°ãƒ©ãƒŸ...   \n",
       "2  Näºˆå‚™æ ¡ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°å…¥é–€ã‚³ãƒ¼ã‚¹ã¯ã‚´ãƒ¼ãƒ«ãŒã€Webã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã¨ã—ã¦å†…å®šã‚‚ã‚‰ã£ã¦åƒã‘ã‚‹ã€ãƒ¬ãƒ™...   \n",
       "\n",
       "                                 1279267537629278211  \\\n",
       "0  AR/VRã«ã¯å¤§ããªãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ãŒã‚ã‚‹ã¨æ€ã„ã¾ã™ãŒã€Oculus Questã«ç‰¹åŒ–ã—ãŸã‚³ãƒ³ãƒ†...   \n",
       "1  #ã‚®ãƒ¼ã‚¯ãƒã‚¦ã‚¹ å“å·ãƒ»äº”åç”°ãƒ»å¤§å´ã§ã¯ä½äººã‚’å‹Ÿé›†ä¸­!\\n\\nã‚³ãƒ³ã‚»ãƒ—ãƒˆã¯ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢åŒå£«(å¿—æœ›...   \n",
       "2  ã€2021å¹´æœ€æ–°ç‰ˆã€‘\\n å›½å†…æœ€å¤§æ‰‹ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚¹ã‚¯ãƒ¼ãƒ«\\nä¾ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å¡¾ã®æ°—ã«ãªã‚‹æ–™é‡‘...   \n",
       "\n",
       "                                 1282640602396364802  \\\n",
       "0  ç§ã¨è¿«ã•ã‚“ï¼ˆ@yuki_99_sï¼‰ã®YouTubeè£ãƒã‚¦ãƒã‚¦Brainã§ã™ãŒã€3ã‚«æœˆä»¥ä¸ŠçµŒã£...   \n",
       "1  ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™â˜€ï¸\\n\\nã€Œèª°ã§ã‚‚ã§ãã‚‹ã“ã¨ã€ã‚’ã€Œèª°ã‚‚ã§ããªã„ãã‚‰ã„ç¶šã‘ã‚‹ã€ãŒæˆåŠŸã®ã‚³ãƒ„...   \n",
       "2  è‰²ã€…ã¨è¨˜äº‹ã«ã—ã¦å…±æœ‰ã—ã¦ã„ããŸã„çŸ¥è­˜ãŒã“ã“1å¹´ã§ãŸãã•ã‚“èº«ã«ã¤ã„ãŸæ°—ãŒã—ã¾ã™ï¼\\n\\nã„ã¾è¡Œ...   \n",
       "\n",
       "                                 1290624958603829248  \\\n",
       "0  2021/11/08 8æ—¥ç›® ä½œæ¥­æ™‚é–“:1h30m\\n\\nãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–Dog3ã¾ã§å®Œæˆï¼\\n...   \n",
       "1  #ãƒ‡ã‚¤ãƒˆãƒ©\\nåˆ†ã‹ã‚‰ãªã„ã¨ã“ã‚ã‚¬ãƒ³ã‚¬ãƒ³å‘Ÿã„ã¦ã„ãã¾ã™ğŸ˜µâ€ğŸ’«\\n\\nä¸­ç´šç·¨ã§ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–ã®\\n...   \n",
       "2  å®Ÿæˆ¦ã—ãŸä¸­ã«ã‹ãªã‚ŠåŠ¹æœçš„ãªå‹‰å¼·ã¯\\nã‚´ãƒ¼ãƒ«ã‹ã‚‰å§‹ã‚ã‚‹äº‹ã§ã™ï¼\\nè³‡æ ¼ãªã‚‰éå»å•ã‹ã‚‰è§£ã\\nåˆ¶...   \n",
       "\n",
       "                                 1297906406155104256  \\\n",
       "0  ã€#tomoya_yoshimoto_in_isub #isubã€‘from slack\\nå¿«...   \n",
       "1  è‡ªåˆ†ã¯ã¿ã‚“ãªã‚’åŠ©ã‘ãŸã„ã¨è¨€ã£ã¦ã‚‚ã€çµå±€èª°ã‚‚äººãŒåŠ©ã‘ã‚’æ±‚ã‚ã¦æ¥ãªã„ã®ãªã‚‰ãã‚Œã¯ã¾ãšãã®äººã«äººã‚’...   \n",
       "2  ãƒˆãƒªãƒ‹ã‚¿ã‚¹ãŠä»•äº‹æƒ…å ±â™ªé«˜é¡æ¡ˆä»¶ã‚ã‚Šâ™ªã‚¹ã‚­ãƒ«ã‚¢ãƒƒãƒ—â™ªé¸ã¹ã‚‹æ¡ˆä»¶â™ªã€€https://t.co/t...   \n",
       "\n",
       "                                          1356973982  \\\n",
       "0  é€†é‹å‹•å­¦ã«æ‚©ã‚€ãƒ»ãƒ»ãƒ»\\nhttps://t.co/U1vksBjNQi\\n\\n#Arduin...   \n",
       "1  é ­æ‚ªã„å¥´ã¯ #ã‚²ãƒ¼ãƒ  ã ã‚ã†ãŒæ”»ç•¥æ³•è¦‹ã¦ã‚Šã‚ƒè‰¯ã„\\n\\n#ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚° ãªã©å°‚é–€çŸ¥è­˜ã«ã—ã¦...   \n",
       "2  2021-11-08 17:00:02\\nä»™å°å¸‚\\nç¾åœ¨ã®å¤©å€™ï¼šæ‰€ã«ã‚ˆã‚Šæ›‡ã‚Š\\nã€€ã€€ã€€æ°—æ¸©ï¼š...   \n",
       "\n",
       "                                          1414520142  \\\n",
       "0  é›¢è·ç‡ã®é«˜ã„ä¼šç¤¾ã£ã¦ã™ã”ã„ãƒãƒ£ãƒ³ã‚¹ã ã£ãŸã‚Šã™ã‚‹ã€‚\\n\\nå…ˆè¼©ã‚„ä¸Šå¸ãŒã©ã‚“ã©ã‚“è¾ã‚ã¦ã„ãã®ã§ã€...   \n",
       "1  æŒã¤ã¹ãã¯çµ¶å¯¾çš„ãªå±æ©Ÿæ„Ÿ\\nä¼šç¤¾å“¡æ™‚ä»£ã«ã¯çµ‚èº«é›‡ç”¨ã§å®‰æ³°ã¨æ€ã‚ãªã„ã“ã¨ã€‚å¸¸ã«ç–‘å•ç¬¦ã‚’ã‚‚ã¡10...   \n",
       "2  #Qiita æŠ•ç¨¿ã—ã¾ã—ãŸã€‚\\n\\nhttps://t.co/cTLwZgbcWY\\n\\n#...   \n",
       "\n",
       "                                 1415640388241555461  \\\n",
       "0  #ä»Šæ—¥ã®ç©ã¿ä¸Šã’\\nä»Šæ—¥ã¯ã‚¨ãƒŠã‚¸ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ã®ç¼¶ã‚’ç©ã¿ã¾ã—ãŸâ€¼ï¸ ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚¹ã¨ã—ã¦ç‹¬ç«‹ã—ã¦ã„...   \n",
       "1  [From ï¾•ï¾ƒï¾ï¾€ï¾ï½¸ï¾ï½«bot]\\nğŸ£ #ä»Šæ—¥ã®ç©ã¿ä¸Šã’\\n#é§†ã‘å‡ºã—ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã¨ç¹‹ãŒã‚Š...   \n",
       "2  ãƒ–ãƒ³ãƒ»ãƒ¬ãƒ¢ãƒ³ã‚°ãƒ©ã‚¹ ãƒã‚­ãƒ³ã€ãƒ™ãƒˆãƒ‹ãƒ£ãƒƒãƒˆã€æ–°å®¿å¾¡è‹‘ #ãŠã¯æˆ¦31102nk #ãŠã¤ã‹ã‚Œæˆ¦éšŠ1...   \n",
       "\n",
       "                                 1449028461310332936  \\\n",
       "0  æ˜¨æ—¥ä»Šæ—¥ã¨åœé›»&amp;ãƒãƒƒãƒˆçµ¶ä¸èª¿ã§ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°å­¦ç¿’ã¯ãŠã‚ã‹ã€SNSã•ãˆã¾ã¨ã‚‚ã«ã§ããªã„...   \n",
       "1  Progateã§ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°å‹‰å¼·å§‹ã‚ã¾ã—ãŸï¼ï¼\\nHTMLï¼†CSSã€€åˆç´šç·¨ã‚„ã£ã¦ã¾ã™ï¼\\n...   \n",
       "2  PHPã®è³ªå•ã«ã¾ã ç­”ãˆã‚‰ã‚Œãªãã¦æ‚²ã—ã„ğŸ’¦\\nçŸ¥ã‚Šåˆã„ã§PHPã®ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã•ã‚“ã¨ã‹ã„ã‚Œã°ã‚ˆã‹ã£...   \n",
       "\n",
       "                                  963753537053143042  \n",
       "0  Railsã€€Modelã§æ•´æ•°å€¤ã«é™å®šã—ãŸã‘ã°\\nvalidates :user_id, nu...  \n",
       "1  ğŸ™‡ğŸ»â€â™‚ï¸å¾¡ç¤¼ğŸ™‡ğŸ»â€â™‚ï¸\\n\\nã‚¯ãƒ©ã‚¦ãƒ‰ãƒ¯ãƒ¼ã‚¯ã‚¹ã§ä½•åº¦å¿œå‹Ÿã—ã¦ã‚‚ãƒ€ãƒ¡ãªäººã®ãŸã‚ã®Brainï¼ˆ...  \n",
       "2  ï¼\\nğŸ”°æœªçµŒé¨“è€…å¿…è¦‹ï¼\\nï¼¼\\n\\næœªçµŒé¨“ã‹ã‚‰ã®ITè»¢è·ã¯20ä»£ãŒæœ‰åˆ©ï¼ğŸš€\\nè»¢è·ã™ã‚‹ãŸã‚ã«...  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ã“ã®ã‚»ãƒ«ã®df_liked_tweetså¤‰æ•°ã®ä¸­èº«ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§æ®‹ã£ã¦ã‚‹ã‚‚ã®\n",
    "# ãƒªã‚¹ã‚¿ãƒ¼ãƒˆã—ãŸã‚‰æ¶ˆãˆã‚‹\n",
    "\n",
    "user_ids = df_liked_tweets.columns.get_level_values(0).unique()\n",
    "\n",
    "tweets_text_list = []\n",
    "for ids in user_ids:\n",
    "    user_texts = df_liked_tweets[ids]['text'].copy()\n",
    "    # ã„ã„ã­ã—ãŸãƒ„ã‚¤ãƒ¼ãƒˆã«ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ„ã‚¤ãƒ¼ãƒˆãŒå«ã¾ã‚Œã¦ã„ãŸã‚‰NANã«ç½®æ›ã™ã‚‹\n",
    "    user_texts.replace(twlist[1], np.nan, inplace=True)\n",
    "    tweets_text_list.append(user_texts)\n",
    "    # ã„ã„ã­ã—ãŸãƒ„ã‚¤ãƒ¼ãƒˆã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ \n",
    "df_tweets_text = pd.DataFrame(tweets_text_list, index=user_ids).T\n",
    "df_tweets_text.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ãƒ„ã‚¤ãƒ¼ãƒˆæ–‡ä»¥å¤–ã¯NANã«ç½®æ›ã™ã‚‹\n",
    "keyword = '#GoogleAppsScript'\n",
    "for col in df_tweets_text.columns:\n",
    "    flag = df_tweets_text[col].str.contains(keyword, na=False)\n",
    "    df_tweets_text.loc[~flag, col] = np.nan\n",
    "\n",
    "# NANä»¥å¤–ã®å€¤ã‚’æŒã¤ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’ç‰¹å®šã™ã‚‹ï¼ˆPOSTã™ã‚‹å€™è£œï¼‰\n",
    "exist_flag = df_tweets_text.count() != 0\n",
    "user_name = df_tweets_text.T[exist_flag].index\n",
    "\n",
    "# POSTã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼å€™è£œã”ã¨ã«é–¢é€£ãƒ„ã‚¤ãƒ¼ãƒˆæ–‡ã‚’å…¨ã¦çµåˆ\n",
    "candidate_users = df_tweets_text[user_name].T\n",
    "# å¾Œã§æ–‡ã‚’åŒºåˆ‡ã‚‹åŒºåˆ‡ã‚Šã®ãŸã‚ãƒ„ã‚¤ãƒ¼ãƒˆæ–‡ã®æ–‡æœ«ã«'ã€‚'ã‚’è¿½åŠ \n",
    "candidate_users += 'ã€‚'\n",
    "# ãƒ„ã‚¤ãƒ¼ãƒˆæ–‡ã‚’çµåˆ\n",
    "candidate_users['all'] = candidate_users.iloc[:,0].str.cat([candidate_users.iloc[:,col] for col in candidate_users.columns[1:]], na_rep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1282640602396364802    2ã¤ã¨ã‚‚ä½¿ãˆã‚‹ã‚ˆã†ã«ãªã‚ŠãŸã„ğŸ¤”ğŸ¤”ww\\n\\n#ä½¿ã„ãŸã„æ§‹æ–‡ #JavaScript #Goo...\n",
       "1356973982             äºŒæ¬¡å…ƒé…åˆ—ã‹ã‚‰ç‰¹å®šã®åˆ—ã ã‘å–ã‚Šå‡ºã™æ–¹æ³•\\n\\nã“ã‚Œã‚ã¡ã‚ƒä¾¿åˆ©ğŸ¤”ğŸ¤”\\n\\nconst pick...\n",
       "Name: all, dtype: object"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "can_user = candidate_users['all']\n",
    "can_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_punc2 = functools.partial(split_punctuation, punctuations=r\"ã€‚!?\")\n",
    "concat_tail_no = functools.partial(concatenate_matching, former_matching_rule=r\"^(?P<result>.+)(ã®)$\", remove_former_matched=False)\n",
    "segmenter = make_pipeline(normalize, split_newline, concat_tail_no, split_punc2)\n",
    "\n",
    "# çµåˆã—ãŸãƒ„ã‚¤ãƒ¼ãƒˆæ–‡ã‚’æ–‡å˜ä½ã§åˆ†å‰²ã™ã‚‹\n",
    "sentence_list = []\n",
    "for text in can_user:\n",
    "    sentence_list.append(list(segmenter(text)))\n",
    "\n",
    "#can_user['text'] = df_list\n",
    "can_user_sentence = pd.Series(sentence_list, index=can_user.index, name='text')\n",
    "\n",
    "# 'ã€‚'ã ã‘ã®æ–‡å­—ã¨çµµæ–‡å­—ã‚’å‰Šé™¤\n",
    "\n",
    "for i in range(len(can_user_sentence)):\n",
    "    can_user_sentence[i] = [demoji.replace(string=seg, repl='') for seg in can_user_sentence[i] if 'ã€‚' != seg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2ã¤ã¨ã‚‚ä½¿ãˆã‚‹ã‚ˆã†ã«ãªã‚ŠãŸã„ww',\n",
       " '#ä½¿ã„ãŸã„æ§‹æ–‡#JavaScript#GoogleAppsScript ',\n",
       " '#filter#é…åˆ—',\n",
       " '#ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°#ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°åˆå¿ƒè€…ã¨ç¹‹ãŒã‚ŠãŸã„#é§†ã‘å‡ºã—ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã¨ç¹‹ãŒã‚ŠãŸã„#é§†ã‘å‡ºã—ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢https://t.co/Ne6kOA0MVwã€‚']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "can_user_sentence[0][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2ã¤ã¨ã‚‚ä½¿ãˆã‚‹ã‚ˆã†ã«ãªã‚ŠãŸã„ğŸ¤”ğŸ¤”ww',\n",
       " '#ä½¿ã„ãŸã„æ§‹æ–‡#JavaScript#GoogleAppsScript ',\n",
       " '#filter#é…åˆ—',\n",
       " '#ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°#ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°åˆå¿ƒè€…ã¨ç¹‹ãŒã‚ŠãŸã„#é§†ã‘å‡ºã—ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã¨ç¹‹ãŒã‚ŠãŸã„#é§†ã‘å‡ºã—ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢https://t.co/Ne6kOA0MVwã€‚']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "can_user_sentence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1282640602396364802</th>\n",
       "      <td>[2ã¤ã¨ã‚‚ä½¿ãˆã‚‹ã‚ˆã†ã«ãªã‚ŠãŸã„ww, #ä½¿ã„ãŸã„æ§‹æ–‡#JavaScript#GoogleAp...</td>\n",
       "      <td>0    [-0.18843903, -0.19478449, -0.020317644, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356973982</th>\n",
       "      <td>[äºŒæ¬¡å…ƒé…åˆ—ã‹ã‚‰ç‰¹å®šã®åˆ—ã ã‘å–ã‚Šå‡ºã™æ–¹æ³•, ã“ã‚Œã‚ã¡ã‚ƒä¾¿åˆ©, const pickedï¼ht...</td>\n",
       "      <td>0    [-0.14769673, -0.20070626, -0.4178725, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text  \\\n",
       "1282640602396364802  [2ã¤ã¨ã‚‚ä½¿ãˆã‚‹ã‚ˆã†ã«ãªã‚ŠãŸã„ww, #ä½¿ã„ãŸã„æ§‹æ–‡#JavaScript#GoogleAp...   \n",
       "1356973982           [äºŒæ¬¡å…ƒé…åˆ—ã‹ã‚‰ç‰¹å®šã®åˆ—ã ã‘å–ã‚Šå‡ºã™æ–¹æ³•, ã“ã‚Œã‚ã¡ã‚ƒä¾¿åˆ©, const pickedï¼ht...   \n",
       "\n",
       "                                                                vector  \n",
       "1282640602396364802  0    [-0.18843903, -0.19478449, -0.020317644, ...  \n",
       "1356973982           0    [-0.14769673, -0.20070626, -0.4178725, -0...  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_candidate_segments(can_user_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d2d66490f4cee6a5e888806b9e6f453dddbe2283d64397fcec40d4118eb281a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('streamlit-custom': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
